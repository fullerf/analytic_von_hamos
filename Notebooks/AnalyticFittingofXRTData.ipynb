{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "import h5py\n",
    "sys.path.append('/home/fdfuller/work/analytic_von_hamos/') \n",
    "from analytic_von_hamos.raytracing import *\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from analytic_von_hamos.data_extraction import get_peaks_and_intensity as pk\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "tfb = tfp.bijectors\n",
    "import imageio\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                yield pickle.load(file)\n",
    "        except EOFError:\n",
    "            pass\n",
    "        \n",
    "def get_2D_from_pickle(path):\n",
    "    x = next(read_from_pickle(path))\n",
    "    xaxis = np.linspace(x.xlimits[0], x.xlimits[1], x.total2D.shape[0])\n",
    "    yaxis = np.linspace(x.ylimits[0], x.ylimits[1], x.total2D.shape[1])\n",
    "    return x.total2D, xaxis, yaxis\n",
    "\n",
    "def get_manifold_from_pickle(path, percentile=96):\n",
    "    rI, rx, ry = get_2D_from_pickle(path)\n",
    "    f = rI > np.percentile(rI[:],(percentile,))\n",
    "    X, Y = np.meshgrid(rx,ry)\n",
    "    return np.stack([Y[f], X[f]],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARaycingModel(gpflow.base.Module, gpflow.models.InternalDataTrainingLossMixin):\n",
    "# class OneLineFit(gpflow.models.model.GPModel, gpflow.models.InternalDataTrainingLossMixin, Standardizer):\n",
    "    r\"\"\"\n",
    "    Gaussian Process Regression.\n",
    "    This is a vanilla implementation of GP regression with a Gaussian\n",
    "    likelihood.  Multiple columns of Y are treated independently.\n",
    "\n",
    "    The log likelihood of this model is sometimes referred to as the 'log\n",
    "    marginal likelihood', and is given by\n",
    "    .. math::\n",
    "       \\log p(\\mathbf y \\,|\\, \\mathbf f) =\n",
    "            \\mathcal N(\\mathbf{y} \\,|\\, 0, \\mathbf{K} + \\sigma_n \\mathbf{I})\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        energy=None,\n",
    "        a=None, \n",
    "        b=None, \n",
    "        c=None,\n",
    "        p=None, \n",
    "        q=None, \n",
    "        r=None,\n",
    "        t=None, \n",
    "        u=None, \n",
    "        v=None,\n",
    "        radius=None,\n",
    "        mm_to_pixels = 20,\n",
    "        pts_per_iteration = 1000,\n",
    "        theta_min = -np.pi/2.15,\n",
    "        theta_max = np.pi/2.15,\n",
    "        xtal = CrystalSi(4,4,0)\n",
    "    ):\n",
    "#         super().__init__()\n",
    "        \n",
    "        if energy is None:\n",
    "            raise ValueError('must specify initial energy')\n",
    "        if a is None:\n",
    "            raise ValueError('must specify initial a')\n",
    "        if b is None:\n",
    "            raise ValueError('must specify initial b')\n",
    "        if c is None:\n",
    "            raise ValueError('must specify initial c')\n",
    "        if p is None:\n",
    "            raise ValueError('must specify initial p')\n",
    "        if q is None:\n",
    "            raise ValueError('must specify initial q')\n",
    "        if r is None:\n",
    "            raise ValueError('must specify initial r')\n",
    "        if t is None:\n",
    "            raise ValueError('must specify initial t')\n",
    "        if u is None:\n",
    "            raise ValueError('must specify initial u')\n",
    "        if v is None:\n",
    "            raise ValueError('must specify initial v')\n",
    "        if radius is None:\n",
    "            raise ValueError('must specify initial radius')\n",
    "            \n",
    "        self.energy = gpflow.Parameter(energy)\n",
    "        self.a = gpflow.Parameter(float(a))\n",
    "        self.b = gpflow.Parameter(float(b))\n",
    "        self.c = gpflow.Parameter(float(c))\n",
    "        self.p = gpflow.Parameter(float(p))\n",
    "        self.q = gpflow.Parameter(float(q))\n",
    "        self.r = gpflow.Parameter(float(r))    \n",
    "        self.t = gpflow.Parameter(float(t),transform=tfb.Exp())\n",
    "        self.u = gpflow.Parameter(float(u))\n",
    "        self.v = gpflow.Parameter(float(v))\n",
    "        self.radius = tf.convert_to_tensor(radius,dtype=gpflow.default_float())\n",
    "        self.xtal = xtal\n",
    "        self.mm_to_pixels = mm_to_pixels  #convert from mm to det pixel space (20pixels/mm)\n",
    "        self.theta_min = theta_min\n",
    "        self.theta_max = theta_max\n",
    "        self.theta_range = tf.cast(tf.linspace(self.theta_min,\n",
    "                                       self.theta_max,\n",
    "                                       pts_per_iteration),tf.float64)\n",
    "    \n",
    "    def nan_filter(self, energy):\n",
    "        y = dety(self.xtal(energy),self.a,self.b,self.c,self.p,self.q,self.r,\n",
    "                                 self.t,self.u,self.v,self.radius,self.theta_range)\n",
    "        z = detz(self.xtal(energy),self.a,self.b,self.c,self.p,self.q,self.r,\n",
    "                                 self.t,self.u,self.v,self.radius,self.theta_range)\n",
    "        nan_filter_y = tf.math.logical_not(tf.math.is_nan(y))\n",
    "        nan_filter_z = tf.math.logical_not(tf.math.is_nan(z))\n",
    "        return tf.math.logical_and(nan_filter_y, nan_filter_z)\n",
    "    \n",
    "    def raytracing_pts(self, energy) -> tf.Tensor:   #call realspace_raytracing_predictions\n",
    "        nan_filter = tf.stop_gradient(self.nan_filter(energy))\n",
    "        \n",
    "        y = dety(self.xtal(energy),self.a,self.b,self.c,self.p,self.q,self.r,\n",
    "                                 self.t,self.u,self.v,self.radius,self.theta_range[nan_filter])\n",
    "        z = detz(self.xtal(energy),self.a,self.b,self.c,self.p,self.q,self.r,\n",
    "                                 self.t,self.u,self.v,self.radius,self.theta_range[nan_filter])\n",
    "        detector_pts = tf.concat([y[:,None], z[:,None]],-1)\n",
    "        return detector_pts\n",
    "    \n",
    "    def __call__(self):\n",
    "        if len(self.energy.get_shape()) > 0:\n",
    "            pts = [self.raytracing_pts(energy)[None,:,:] for energy in self.energy]\n",
    "            pts = tf.concat(pts, 0)\n",
    "        else:\n",
    "            pts = self.raytracing_pts(self.energy)\n",
    "        return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 500\n",
    "\n",
    "parameters1 = {\n",
    "    'a': 500,\n",
    "    'b': 0,\n",
    "    'c': -500,\n",
    "    'p': 505,\n",
    "    'q': 0,\n",
    "    'r': 500,\n",
    "    't': 1,\n",
    "    'u': 0,\n",
    "    'v': 0,\n",
    "    'radius': r,\n",
    "    'energy': (9130,9140)\n",
    "}\n",
    "\n",
    "parameters2 = {\n",
    "    'a': 450,\n",
    "    'b': 0,\n",
    "    'c': -500,\n",
    "    'p': 550,\n",
    "    'q': 0,\n",
    "    'r': 500,\n",
    "    't': 1,\n",
    "    'u': 0,\n",
    "    'v': 0,\n",
    "    'radius': r,\n",
    "    'energy': (9130,9140),\n",
    "    'theta_max': np.pi/2.02,\n",
    "    'theta_min': -np.pi/2.02,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-shipping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = ARaycingModel(**parameters1)\n",
    "figure(figsize=(5,5))\n",
    "predicted_detpts = m()\n",
    "manifold_1 = get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9130.pickle', percentile=99)\n",
    "manifold_2 = get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9140.pickle', percentile=99)\n",
    "scatter(manifold_1[:,1], manifold_1[:,0],marker='.',c='k',label='xrt simulation at 9130 eV', alpha=0.5)\n",
    "scatter(manifold_2[:,1], manifold_2[:,0],marker='.',c='g',label='xrt simulation at 9140 eV', alpha=0.5)\n",
    "plot(predicted_detpts[0,:,1],predicted_detpts[0,:,0],'r-',label='ours at 9130 ev')\n",
    "plot(predicted_detpts[1,:,1],predicted_detpts[1,:,0],'b-',label='ours at 9140 eV')\n",
    "legend(loc=5)\n",
    "xlim([-1,40])\n",
    "ylim([-30,30])\n",
    "ax = gca()\n",
    "ax.set_aspect(3/4)\n",
    "xlabel('Z (mm)')\n",
    "ylabel('Y (mm)')\n",
    "savefig('Geom_agreement_1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ARaycingModel(**parameters2)\n",
    "figure()\n",
    "predicted_detpts = m()\n",
    "manifold_1 = get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9130.pickle', percentile=99)\n",
    "manifold_2 = get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9140.pickle', percentile=99)\n",
    "scatter(manifold_1[:,1], manifold_1[:,0],marker='.',c='k',label='xrt simulation at 9130 eV', alpha=0.5)\n",
    "scatter(manifold_2[:,1], manifold_2[:,0],marker='.',c='g',label='xrt simulation at 9140 eV', alpha=0.5)\n",
    "plot(predicted_detpts[0,:,1],predicted_detpts[0,:,0],'r-',label='ours at 9130 ev')\n",
    "plot(predicted_detpts[1,:,1],predicted_detpts[1,:,0],'b-',label='ours at 9140 ev')\n",
    "legend(loc=5)\n",
    "xlim([-15,30])\n",
    "ylim([-10,10])\n",
    "# xlim([-50,50])\n",
    "# ylim([-50,50])\n",
    "ax = gca()\n",
    "ax.set_aspect(3/4)\n",
    "xlabel('Z (mm)')\n",
    "ylabel('Y (mm)')\n",
    "# savefig('cartoid_optim\n",
    "\n",
    "\n",
    "\n",
    "# scatter(manifold_2[:,1], manifold_2[:,0],marker='.',c='k',label='xrt simulation')\n",
    "# plot(predicted_detpts[0,:,1],predicted_detpts[0,:,0],'r-',label='9130 ev')\n",
    "# plot(predicted_detpts[1,:,1],predicted_detpts[1,:,0],'b-',label='9140 eV')\n",
    "# legend(loc=6)\n",
    "# xlim([-15,5])\n",
    "# ylim([-10,10])\n",
    "# ax = gca()\n",
    "# ax.set_aspect(3/4)\n",
    "# xlabel('Z (mm)')\n",
    "# ylabel('Y (mm)')\n",
    "savefig('Geom_agreement_2.pdf')\n",
    "\n",
    "\n",
    "\n",
    "# m = ARaycingModel(**parameters2)\n",
    "\n",
    "\n",
    "# figure()\n",
    "# predicted_detpts = m()\n",
    "# rI, rx, ry = get_2D_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500.pickle')\n",
    "# contour(rx, ry, rI, 100)\n",
    "# scatter(predicted_detpts[:,1],predicted_detpts[:,0],c='r',marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_of_basis(t,u,v):\n",
    "    if t.ndim == 1:\n",
    "        t = t[:,None]\n",
    "    if u.ndim == 1:\n",
    "        u = u[:,None]\n",
    "    if v.ndim == 1:\n",
    "        v = v[:,None]\n",
    "    tuv = np.concatenate([np.atleast_2d(t),np.atleast_2d(u),np.atleast_2d(v)],-1)\n",
    "    row_1 = tuv\n",
    "    row_2 = -np.cross(tuv,np.cross((1,0,0), (0,1,0))[None,:])\n",
    "    row_3 = np.cross(tuv, np.array((0,1,0))[None,:])\n",
    "    T = np.concatenate([row_1[:,None,:], row_2[:,None,:], row_3[:,None,:]],1)\n",
    "    return np.linalg.inv(T)\n",
    "\n",
    "def initial_guesses_for_experimental_geometry(N: int):\n",
    "    grand = lambda x0, xu: np.random.uniform(low=x0-xu/2, high=x0+xu/2, size=(N,))\n",
    "    f = 250\n",
    "    c0 = -490\n",
    "    a0 = 500\n",
    "    b0 = 0.\n",
    "    d0 = 2*f*np.sqrt(2)\n",
    "    d_uncertainty = 10\n",
    "    a_uncertainty = 10\n",
    "    b_uncertainty = 10\n",
    "    c_uncertainty = 10\n",
    "    \n",
    "    \n",
    "    def ac_from_dc():\n",
    "        d = grand(d0,d_uncertainty)\n",
    "        c = grand(c0,c_uncertainty)\n",
    "        return np.sqrt(d**2 - c**2), c\n",
    "    a,c = ac_from_dc()\n",
    "    b = grand(b0,b_uncertainty)\n",
    "    \n",
    "    p0 = 515\n",
    "    q0 = 0.0\n",
    "    r0 = 500\n",
    "    p_uncertainty = 10\n",
    "    q_uncertainty = 10\n",
    "    r_uncertainty = 10\n",
    "    \n",
    "    dd0 = 700\n",
    "    p = grand(p0, p_uncertainty)\n",
    "    r = grand(r0, r_uncertainty)\n",
    "    q = grand(q0, q_uncertainty)\n",
    "    \n",
    "    t0 = 1\n",
    "    u0 = 0\n",
    "    v0 = 0\n",
    "    t_uncertainty = 0.005\n",
    "    u_uncertainty = 0.005\n",
    "    v_uncertainty = 0.005\n",
    "    \n",
    "    t = grand(t0,t_uncertainty)\n",
    "    u = grand(u0,u_uncertainty)\n",
    "    v = grand(v0,v_uncertainty)\n",
    "    T = change_of_basis(t,u,v)\n",
    "    \n",
    "    pqr = np.concatenate([p[:,None],q[:,None],r[:,None]],-1)\n",
    "    pqr_prime = np.einsum('ijk,ik->ij',T,pqr)\n",
    "    \n",
    "    pp = pqr_prime[:,0]\n",
    "    qp = pqr_prime[:,1]\n",
    "    rp = pqr_prime[:,2]\n",
    "    if N == 1:\n",
    "        return {'a': float(a), 'b': float(b), 'c': float(c),\n",
    "                'p': float(p), 'q': float(q), 'r': float(r),\n",
    "                't': float(t), 'u': float(u), 'v': float(v)}\n",
    "    else:\n",
    "        return {'a': a, 'b': b, 'c': c, 'p': p, 'q': q, 'r': r, 't': t, 'u': u, 'v': v}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "{**initial_guesses_for_experimental_geometry(1), 'energy': 9130, 'radius': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = initial_guesses_for_experimental_geometry(250)\n",
    "m = TwoLineFit(get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9130.pickle', percentile=99),\n",
    "               get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9140.pickle', percentile=99),\n",
    "               **{**initial_guess,\n",
    "                                              'radius': 500,\n",
    "                                              'energy1': 9130,\n",
    "                                              'energy2': 9140,\n",
    "                                              'mm_to_pixels': 1.,\n",
    "                                              'xtal': CrystalSi(4,4,0),\n",
    "                                              'theta_min': -np.pi/2.2,\n",
    "                                              'theta_max': np.pi/2.2,\n",
    "                                             })\n",
    "\n",
    "opt_i0 = tf.argmin(m._individual_training_losses())\n",
    "opt_loss0 = m._individual_training_losses()[opt_i0]\n",
    "p0 = m.get_params(opt_i0)\n",
    "print('initial best guess loss: ', opt_loss0)\n",
    "\n",
    "def plot_loss(loss_log, best_loss_log):\n",
    "    figure(figsize=(10, 4))\n",
    "    title('Loss history')\n",
    "    semilogy(loss_log, 'k-.',alpha=0.3)\n",
    "    figure(figsize=(10, 4))\n",
    "    title('Best Loss history')\n",
    "    semilogy(best_loss_log, 'r-.',alpha=0.9)\n",
    "    show()\n",
    "    \n",
    "lr = 1E-2\n",
    "# lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     [1000, 2000, 3000, 4000, 5000], [lr, lr*0.5, lr*(0.5**2), lr*(0.5**3), lr*(0.5**4), lr*(0.5**5)])\n",
    "lr_sched = tf.keras.experimental.CosineDecayRestarts(lr, 1000, alpha=0.1, m_mul=0.9, t_mul=1.0)\n",
    "trainer = tf.keras.optimizers.Adam(lr_sched)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = m.training_loss()\n",
    "    grads = g.gradient(loss, m.trainable_variables)\n",
    "#     grads = [g/(tf.norm(g)+1e-8) for g in grads]\n",
    "    trainer.apply_gradients(zip(grads, m.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "loss_log = []\n",
    "best_loss_log = []\n",
    "prediction_log = []\n",
    "M = 100\n",
    "for i in range(3000):\n",
    "    loss = train_step()\n",
    "    loss_log.append(loss.numpy())\n",
    "    step_i = len(loss_log)\n",
    "    if step_i%10 == 0:\n",
    "        prediction_log.append(m.raytracing_slice(m.energy1, 20))\n",
    "    if step_i%100 == 0:\n",
    "        clear_output()\n",
    "        indiv_losses = m._individual_training_losses()\n",
    "        best_loss_ind = tf.argmin(indiv_losses)\n",
    "        best_loss_log.append(indiv_losses[best_loss_ind].numpy().item())\n",
    "        plot_loss(loss_log, best_loss_log)\n",
    "    if step_i%150 == 0:\n",
    "        # prune the bad ones\n",
    "        s = tf.argsort(m._individual_training_losses()).numpy()\n",
    "        # replace bad ones with scrambled versions of the top 20 particles\n",
    "        ap = np.concatenate([m.a.numpy()[s[:-M]], m.a.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        bp = np.concatenate([m.b.numpy()[s[:-M]], m.b.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        cp = np.concatenate([m.c.numpy()[s[:-M]], m.c.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        pp = np.concatenate([m.p.numpy()[s[:-M]], m.p.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        qp = np.concatenate([m.q.numpy()[s[:-M]], m.q.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        rp = np.concatenate([m.r.numpy()[s[:-M]], m.r.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        tp = np.concatenate([m.t.numpy()[s[:-M]], m.t.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        up = np.concatenate([m.u.numpy()[s[:-M]], m.u.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        vp = np.concatenate([m.v.numpy()[s[:-M]], m.v.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        m.a.assign(ap)\n",
    "        m.b.assign(bp)\n",
    "        m.c.assign(cp)\n",
    "        m.p.assign(pp)\n",
    "        m.q.assign(qp)\n",
    "        m.r.assign(rp)\n",
    "        m.t.assign(tp)\n",
    "        m.u.assign(up)\n",
    "        m.v.assign(vp)\n",
    "    print('\\r step: %d, loss: %.3f'%(len(loss_log), loss), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9000):\n",
    "    loss = train_step()\n",
    "    loss_log.append(loss.numpy())\n",
    "    step_i = len(loss_log)\n",
    "    if step_i%10 == 0:\n",
    "        prediction_log.append(m.raytracing_slice(m.energy1, 20))\n",
    "    if step_i%100 == 0:\n",
    "        clear_output()\n",
    "        indiv_losses = m._individual_training_losses()\n",
    "        best_loss_ind = tf.argmin(indiv_losses)\n",
    "        best_loss_log.append(indiv_losses[best_loss_ind].numpy().item())\n",
    "        plot_loss(loss_log, best_loss_log)\n",
    "    if step_i%150 == 0:\n",
    "        # prune the bad ones\n",
    "        s = tf.argsort(m._individual_training_losses()).numpy()\n",
    "        # replace bad ones with scrambled versions of the top 20 particles\n",
    "        ap = np.concatenate([m.a.numpy()[s[:-M]], m.a.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        bp = np.concatenate([m.b.numpy()[s[:-M]], m.b.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        cp = np.concatenate([m.c.numpy()[s[:-M]], m.c.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        pp = np.concatenate([m.p.numpy()[s[:-M]], m.p.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        qp = np.concatenate([m.q.numpy()[s[:-M]], m.q.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        rp = np.concatenate([m.r.numpy()[s[:-M]], m.r.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        tp = np.concatenate([m.t.numpy()[s[:-M]], m.t.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        up = np.concatenate([m.u.numpy()[s[:-M]], m.u.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        vp = np.concatenate([m.v.numpy()[s[:-M]], m.v.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        m.a.assign(ap)\n",
    "        m.b.assign(bp)\n",
    "        m.c.assign(cp)\n",
    "        m.p.assign(pp)\n",
    "        m.q.assign(qp)\n",
    "        m.r.assign(rp)\n",
    "        m.t.assign(tp)\n",
    "        m.u.assign(up)\n",
    "        m.v.assign(vp)\n",
    "    print('\\r step: %d, loss: %.3f'%(len(loss_log), loss), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.stack(prediction_log,0)\n",
    "filenames = []\n",
    "for k in range(8,300):\n",
    "    plot(preds[k,0,:,1],preds[k,0,:,0],'r-')\n",
    "    plot(preds[k,1,:,1],preds[k,1,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,2,:,1],preds[k,2,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,3,:,1],preds[k,3,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,4,:,1],preds[k,4,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,5,:,1],preds[k,5,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,6,:,1],preds[k,6,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,7,:,1],preds[k,7,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,8,:,1],preds[k,8,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,9,:,1],preds[k,9,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,10,:,1],preds[k,10,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,11,:,1],preds[k,11,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,12,:,1],preds[k,12,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,13,:,1],preds[k,13,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,14,:,1],preds[k,14,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,15,:,1],preds[k,15,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,16,:,1],preds[k,16,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,17,:,1],preds[k,17,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,18,:,1],preds[k,18,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,19,:,1],preds[k,19,:,0],'k-', alpha=0.1)\n",
    "    xlim([-1,30])\n",
    "    ylim([-30,30])\n",
    "    ax = gca()\n",
    "    ax.set_aspect(3/4)\n",
    "    xlabel('Z (mm)')\n",
    "    ylabel('Y (mm)')\n",
    "    \n",
    "    # create file name and append it to a list\n",
    "    filename = f'{k}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    savefig(filename)\n",
    "    close()# build gif\n",
    "    \n",
    "with imageio.get_writer('optimization_vs.gif', mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "# Remove files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "semilogy(best_loss_log[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_i = tf.argmin(m._individual_training_losses())\n",
    "print(m._individual_training_losses()[opt_i])\n",
    "m.get_params(opt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_i = tf.argmin(m._individual_training_losses())\n",
    "print(m._individual_training_losses()[opt_i])\n",
    "ma = ARaycingModel(**{**m.get_params(opt_i)[0], 'energy': (m.energy1,), 'theta_min': -np.pi/2.2, 'theta_max': np.pi/2.2})\n",
    "mb = ARaycingModel(**{**m.get_params(opt_i)[0], 'energy': (m.energy2,), 'theta_min': -np.pi/2.2, 'theta_max': np.pi/2.2})\n",
    "figure(figsize=(5,5))\n",
    "predicted_detpts1 = ma()\n",
    "predicted_detpts2 = mb()\n",
    "manifold_1 = get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9130.pickle', percentile=99)\n",
    "manifold_2 = get_manifold_from_pickle('../XRT/output_at_p=505_r=500_a=500_c=-500_e=9140.pickle', percentile=99)\n",
    "scatter(manifold_1[:,1], manifold_1[:,0],marker='.',c='k',label='xrt simulation at 9130 eV', alpha=0.5)\n",
    "scatter(manifold_2[:,1], manifold_2[:,0],marker='.',c='g',label='xrt simulation at 9140 eV', alpha=0.5)\n",
    "plot(predicted_detpts1[0,:,1],predicted_detpts1[0,:,0],'r-',label='optimized 9130 ev')\n",
    "plot(predicted_detpts2[0,:,1],predicted_detpts2[0,:,0],'b-',label='optimized 9140 ev')\n",
    "# plot(fit_line1[opt_i,:,1],fit_line1[opt_i,:,0],'r',label='9130 ev')\n",
    "# plot(fit_line2[opt_i,:,1],fit_line2[opt_i,:,0],'b',label='9140 ev')\n",
    "legend(loc=5)\n",
    "xlim([-1,40])\n",
    "ylim([-30,30])\n",
    "ax = gca()\n",
    "ax.set_aspect(3/4)\n",
    "xlabel('Z (mm)')\n",
    "ylabel('Y (mm)')\n",
    "# savefig('vs_optimized.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_guesses_for_experimental_geometry2(N: int):\n",
    "    grand = lambda x0, xu: np.random.uniform(low=x0-xu/2, high=x0+xu/2, size=(N,))\n",
    "    f = 250\n",
    "    c0 = -500\n",
    "    a0 = 435\n",
    "    b0 = 0.\n",
    "    d0 = np.sqrt(a0**2 + c0**2)\n",
    "    d_uncertainty = 10\n",
    "    a_uncertainty = 10\n",
    "    b_uncertainty = 10\n",
    "    c_uncertainty = 10\n",
    "    \n",
    "    \n",
    "    def ac_from_dc():\n",
    "        d = grand(d0,d_uncertainty)\n",
    "        c = grand(c0,c_uncertainty)\n",
    "        return np.sqrt(d**2 - c**2), c\n",
    "    a,c = ac_from_dc()\n",
    "    b = grand(b0,b_uncertainty)\n",
    "    \n",
    "    p0 = 535\n",
    "    q0 = 0.0\n",
    "    r0 = 500\n",
    "    q_uncertainty = 10\n",
    "    r_uncertainty = 10\n",
    "    p_uncertainty = 20\n",
    "    \n",
    "    p = grand(p0, p_uncertainty)\n",
    "    r = grand(r0, r_uncertainty)\n",
    "    q = grand(q0, q_uncertainty)\n",
    "    \n",
    "    t0 = 1\n",
    "    u0 = 0\n",
    "    v0 = 0\n",
    "    t_uncertainty = 0.005\n",
    "    u_uncertainty = 0.005\n",
    "    v_uncertainty = 0.005\n",
    "    \n",
    "    t = grand(t0,t_uncertainty)\n",
    "    u = grand(u0,u_uncertainty)\n",
    "    v = grand(v0,v_uncertainty)\n",
    "    T = change_of_basis(t,u,v)\n",
    "    \n",
    "    pqr = np.concatenate([p[:,None],q[:,None],r[:,None]],-1)\n",
    "    pqr_prime = np.einsum('ijk,ik->ij',T,pqr)\n",
    "    \n",
    "    pp = pqr_prime[:,0]\n",
    "    qp = pqr_prime[:,1]\n",
    "    rp = pqr_prime[:,2]\n",
    "    if N == 1:\n",
    "        return {'a': float(a), 'b': float(b), 'c': float(c),\n",
    "                'p': float(p), 'q': float(q), 'r': float(r),\n",
    "                't': float(t), 'u': float(u), 'v': float(v)}\n",
    "    else:\n",
    "        return {'a': a, 'b': b, 'c': c, 'p': p, 'q': q, 'r': r, 't': t, 'u': u, 'v': v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = initial_guesses_for_experimental_geometry2(250)\n",
    "m2 = TwoLineFit(get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9130.pickle', percentile=99),\n",
    "               get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9140.pickle', percentile=99),\n",
    "               **{**initial_guess,\n",
    "                                              'radius': 500,\n",
    "                                              'energy1': 9130,\n",
    "                                              'energy2': 9140,\n",
    "                                              'mm_to_pixels': 1.,\n",
    "                                              'xtal': CrystalSi(4,4,0),\n",
    "                                              'theta_min': -np.pi/2.02,\n",
    "                                              'theta_max': np.pi/2.02,\n",
    "                                             })\n",
    "\n",
    "opt_i0 = tf.argmin(m2._individual_training_losses())\n",
    "opt_loss0 = m2._individual_training_losses()[opt_i0]\n",
    "p0 = m2.get_params(opt_i0)\n",
    "print('initial best guess loss: ', opt_loss0)\n",
    "\n",
    "def plot_loss(loss_log, best_loss_log):\n",
    "    figure(figsize=(10, 4))\n",
    "    title('Loss history')\n",
    "    semilogy(loss_log, 'k-.',alpha=0.3)\n",
    "    figure(figsize=(10, 4))\n",
    "    title('Best Loss history')\n",
    "    semilogy(best_loss_log, 'r-.',alpha=0.9)\n",
    "    show()\n",
    "    \n",
    "lr = 1E-2\n",
    "# lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     [1000, 2000, 3000, 4000, 5000], [lr, lr*0.5, lr*(0.5**2), lr*(0.5**3), lr*(0.5**4), lr*(0.5**5)])\n",
    "lr_sched = tf.keras.experimental.CosineDecayRestarts(lr, 1000, alpha=0.1, m_mul=0.9, t_mul=1.0)\n",
    "trainer = tf.keras.optimizers.Adam(lr_sched)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = m2.training_loss()\n",
    "    grads = g.gradient(loss, m2.trainable_variables)\n",
    "#     grads = [g/(tf.norm(g)+1e-8) for g in grads]\n",
    "    trainer.apply_gradients(zip(grads, m2.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "loss_log = []\n",
    "best_loss_log = []\n",
    "prediction_log = []\n",
    "M = 100\n",
    "for i in range(12000):\n",
    "    loss = train_step()\n",
    "    loss_log.append(loss.numpy())\n",
    "    step_i = len(loss_log)\n",
    "    if step_i%10 == 0:\n",
    "        prediction_log.append(m2.raytracing_slice(m2.energy1, 20))\n",
    "    if step_i%100 == 0:\n",
    "        clear_output()\n",
    "        indiv_losses = m2._individual_training_losses()\n",
    "        best_loss_ind = tf.argmin(indiv_losses)\n",
    "        best_loss_log.append(indiv_losses[best_loss_ind].numpy().item())\n",
    "        plot_loss(loss_log, best_loss_log)\n",
    "    if step_i%150 == 0:\n",
    "        # prune the bad ones\n",
    "        s = tf.argsort(m2._individual_training_losses()).numpy()\n",
    "        # replace bad ones with scrambled versions of the top 20 particles\n",
    "        ap = np.concatenate([m2.a.numpy()[s[:-M]], m2.a.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        bp = np.concatenate([m2.b.numpy()[s[:-M]], m2.b.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        cp = np.concatenate([m2.c.numpy()[s[:-M]], m2.c.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        pp = np.concatenate([m2.p.numpy()[s[:-M]], m2.p.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        qp = np.concatenate([m2.q.numpy()[s[:-M]], m2.q.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        rp = np.concatenate([m2.r.numpy()[s[:-M]], m2.r.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        tp = np.concatenate([m2.t.numpy()[s[:-M]], m2.t.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        up = np.concatenate([m2.u.numpy()[s[:-M]], m2.u.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        vp = np.concatenate([m2.v.numpy()[s[:-M]], m2.v.numpy()[s[np.random.choice(M,M)]]],0)\n",
    "        m2.a.assign(ap)\n",
    "        m2.b.assign(bp)\n",
    "        m2.c.assign(cp)\n",
    "        m2.p.assign(pp)\n",
    "        m2.q.assign(qp)\n",
    "        m2.r.assign(rp)\n",
    "        m2.t.assign(tp)\n",
    "        m2.u.assign(up)\n",
    "        m2.v.assign(vp)\n",
    "    print('\\r step: %d, loss: %.3f'%(len(loss_log), loss), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "semilogy(best_loss_log[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_i = tf.argmin(m2._individual_training_losses())\n",
    "print(m2._individual_training_losses()[opt_i])\n",
    "m2.get_params(opt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_i = tf.argmin(m2._individual_training_losses())\n",
    "print(m2._individual_training_losses()[opt_i])\n",
    "m2a = ARaycingModel(**{**m2.get_params(opt_i)[0], 'energy': (m2.energy1,), 'theta_min': -np.pi/2.02, 'theta_max': np.pi/2.02})\n",
    "m2b = ARaycingModel(**{**m2.get_params(opt_i)[0], 'energy': (m2.energy2,), 'theta_min': -np.pi/2.02, 'theta_max': np.pi/2.02})\n",
    "m2a0 = ARaycingModel(**{**p0[0], 'energy': (m2.energy1.numpy().item(),), 'theta_min': -np.pi/2.02, 'theta_max': np.pi/2.02})\n",
    "m2b0 = ARaycingModel(**{**p0[1], 'energy': (m2.energy2.numpy().item(),), 'theta_min': -np.pi/2.02, 'theta_max': np.pi/2.02})\n",
    "figure(figsize=(5,5))\n",
    "predicted_detpts1 = m2a()\n",
    "predicted_detpts2 = m2b()\n",
    "predicted_detpts1_0 = m2a0()\n",
    "predicted_detpts2_0 = m2b0()\n",
    "\n",
    "manifold_1 = get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9130.pickle', percentile=99)\n",
    "manifold_2 = get_manifold_from_pickle('../XRT/output_at_p=550_r=500_a=450_c=-500_e=9140.pickle', percentile=99)\n",
    "scatter(manifold_1[:,1], manifold_1[:,0],marker='.',c='k',label='xrt simulation at 9130 eV', alpha=0.5)\n",
    "scatter(manifold_2[:,1], manifold_2[:,0],marker='.',c='g',label='xrt simulation at 9140 eV', alpha=0.5)\n",
    "plot(predicted_detpts1[0,:,1],predicted_detpts1[0,:,0],'r-',label='optimized 9130 ev')\n",
    "plot(predicted_detpts2[0,:,1],predicted_detpts2[0,:,0],'b-',label='optimized 9140 ev')\n",
    "\n",
    "legend(loc=5)\n",
    "xlim([-15,30])\n",
    "ylim([-10,10])\n",
    "# xlim([-50,50])\n",
    "# ylim([-50,50])\n",
    "ax = gca()\n",
    "ax.set_aspect(3/4)\n",
    "xlabel('Z (mm)')\n",
    "ylabel('Y (mm)')\n",
    "# savefig('cartoid_optimized.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.stack(prediction_log,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "for k in range(300):\n",
    "    plot(preds[k,0,:,1],preds[k,0,:,0],'k-',alpha=0.3)\n",
    "xlim([-15,15])\n",
    "ylim([-10,10])\n",
    "# xlim([-50,50])\n",
    "# ylim([-50,50])\n",
    "ax = gca()\n",
    "ax.set_aspect(3/4)\n",
    "xlabel('Z (mm)')\n",
    "ylabel('Y (mm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.stack(prediction_log,0)\n",
    "filenames = []\n",
    "for k in range(8,300):\n",
    "    plot(preds[k,0,:,1],preds[k,0,:,0],'r-')\n",
    "    plot(preds[k,1,:,1],preds[k,1,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,2,:,1],preds[k,2,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,3,:,1],preds[k,3,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,4,:,1],preds[k,4,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,5,:,1],preds[k,5,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,6,:,1],preds[k,6,:,0],'k-', alpha=0.3)\n",
    "    plot(preds[k,7,:,1],preds[k,7,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,8,:,1],preds[k,8,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,9,:,1],preds[k,9,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,10,:,1],preds[k,10,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,11,:,1],preds[k,11,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,12,:,1],preds[k,12,:,0],'k-', alpha=0.2)\n",
    "    plot(preds[k,13,:,1],preds[k,13,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,14,:,1],preds[k,14,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,15,:,1],preds[k,15,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,16,:,1],preds[k,16,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,17,:,1],preds[k,17,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,18,:,1],preds[k,18,:,0],'k-', alpha=0.1)\n",
    "    plot(preds[k,19,:,1],preds[k,19,:,0],'k-', alpha=0.1)\n",
    "    xlim([-15,15])\n",
    "    ylim([-10,10])\n",
    "    ax = gca()\n",
    "    ax.set_aspect(3/4)\n",
    "    xlabel('Z (mm)')\n",
    "    ylabel('Y (mm)')\n",
    "    \n",
    "    # create file name and append it to a list\n",
    "    filename = f'{k}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    savefig(filename)\n",
    "    close()# build gif\n",
    "    \n",
    "with imageio.get_writer('optimization_cartoid.gif', mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "# Remove files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-brisbane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
